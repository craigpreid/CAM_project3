{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankurgupta/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emotion</td>\n",
       "      <td>pixels</td>\n",
       "      <td>Usage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>85 84 90 121 101 102 133 153 153 169 177 189 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     usage\n",
       "0  emotion                                             pixels     Usage\n",
       "1        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "2        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "3        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "4        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "5        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
       "6        2  55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...  Training\n",
       "7        4  20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...  Training\n",
       "8        3  77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...  Training\n",
       "9        3  85 84 90 121 101 102 133 153 153 169 177 189 1...  Training"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data\n",
    "filname = 'fer2013.csv'\n",
    "label_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "names=['emotion','pixels','usage']\n",
    "df=pd.read_csv('fer2013.csv',names=names, na_filter=False)\n",
    "im=df['pixels']\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(filname):\n",
    "    # images are 48x48\n",
    "    # N = 35887\n",
    "    Y = []\n",
    "    X = []\n",
    "    first = True\n",
    "    n=-1\n",
    "    \n",
    "    for line in open(filname):\n",
    "        n+=1\n",
    "        if n<10001:\n",
    "            if first:\n",
    "                first = False\n",
    "            else:\n",
    "                row = line.split(',')\n",
    "                Y.append(int(row[0]))\n",
    "                X.append([int(p) for p in row[1].split()])\n",
    "\n",
    "    X, Y = np.array(X) / 255.0, np.array(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "X, Y = getData(filname)\n",
    "num_class = len(set(Y))\n",
    "print(num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras with tensorflow backend\n",
    "N, D = X.shape\n",
    "X = X.reshape(N, 48, 48, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 48, 48, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "y_train = (np.arange(num_class) == y_train[:, None]).astype(np.float32)\n",
    "y_test = (np.arange(num_class) == y_test[:, None]).astype(np.float32)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Activation , Dropout ,Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import *\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 48, 48, 64)        1664      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 48, 48, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 24, 24, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 2,787,015\n",
      "Trainable params: 2,785,863\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def my_model():\n",
    "    model = Sequential()\n",
    "    input_shape = (48,48,1)\n",
    "    model.add(Conv2D(64, (5, 5), input_shape=input_shape,activation='relu', padding='same'))\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\n",
    "    model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(7))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    # UNCOMMENT THIS TO VIEW THE ARCHITECTURE\n",
    "    #model.summary()\n",
    "    \n",
    "    return model\n",
    "model=my_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/15\n",
      "9000/9000 [==============================] - 1248s 139ms/step - loss: 1.8515 - acc: 0.2738 - val_loss: 1.9824 - val_acc: 0.1730\n",
      "Epoch 2/15\n",
      "9000/9000 [==============================] - 1265s 141ms/step - loss: 1.6541 - acc: 0.3590 - val_loss: 1.8332 - val_acc: 0.3520\n",
      "Epoch 3/15\n",
      "9000/9000 [==============================] - 2701s 300ms/step - loss: 1.5169 - acc: 0.4151 - val_loss: 1.6605 - val_acc: 0.3290\n",
      "Epoch 4/15\n",
      "9000/9000 [==============================] - 1007s 112ms/step - loss: 1.3888 - acc: 0.4756 - val_loss: 1.5868 - val_acc: 0.3990\n",
      "Epoch 5/15\n",
      "9000/9000 [==============================] - 1063s 118ms/step - loss: 1.2547 - acc: 0.5234 - val_loss: 1.9686 - val_acc: 0.3460\n",
      "Epoch 6/15\n",
      "9000/9000 [==============================] - 6209s 690ms/step - loss: 1.1157 - acc: 0.5802 - val_loss: 1.4831 - val_acc: 0.4680\n",
      "Epoch 7/15\n",
      "9000/9000 [==============================] - 1064s 118ms/step - loss: 0.9676 - acc: 0.6428 - val_loss: 1.6774 - val_acc: 0.4240\n",
      "Epoch 8/15\n",
      "9000/9000 [==============================] - 1172s 130ms/step - loss: 0.7991 - acc: 0.7123 - val_loss: 1.4764 - val_acc: 0.4880\n",
      "Epoch 9/15\n",
      "9000/9000 [==============================] - 1264s 140ms/step - loss: 0.6035 - acc: 0.7940 - val_loss: 1.5897 - val_acc: 0.5080\n",
      "Epoch 10/15\n",
      "9000/9000 [==============================] - 1388s 154ms/step - loss: 0.4509 - acc: 0.8553 - val_loss: 1.6331 - val_acc: 0.4920\n",
      "Epoch 11/15\n",
      "9000/9000 [==============================] - 1381s 153ms/step - loss: 0.2943 - acc: 0.9103 - val_loss: 1.9197 - val_acc: 0.4290\n",
      "Epoch 12/15\n",
      "9000/9000 [==============================] - 1378s 153ms/step - loss: 0.1852 - acc: 0.9501 - val_loss: 1.9950 - val_acc: 0.5050\n",
      "Epoch 13/15\n",
      "9000/9000 [==============================] - 7355s 817ms/step - loss: 0.1277 - acc: 0.9699 - val_loss: 2.1492 - val_acc: 0.4520\n",
      "Epoch 14/15\n",
      "9000/9000 [==============================] - 1114s 124ms/step - loss: 0.0954 - acc: 0.9776 - val_loss: 2.0350 - val_acc: 0.4930\n",
      "Epoch 15/15\n",
      "9000/9000 [==============================] - 2337s 260ms/step - loss: 0.0764 - acc: 0.9826 - val_loss: 2.1656 - val_acc: 0.4960\n"
     ]
    }
   ],
   "source": [
    "path_model='model_filter.h5' # save model at this location after each epoch\n",
    "K.tensorflow_backend.clear_session() # destroys the current graph and builds a new one\n",
    "model=my_model() # create the model\n",
    "K.set_value(model.optimizer.lr,1e-3) # set the learning rate\n",
    "# fit the model\n",
    "h=model.fit(x=X_train,     \n",
    "            y=y_train, \n",
    "            batch_size=64, \n",
    "            epochs=15, \n",
    "            verbose=1, \n",
    "            validation_data=(X_test,y_test),\n",
    "            shuffle=True,\n",
    "            callbacks=[\n",
    "                ModelCheckpoint(filepath=path_model),\n",
    "            ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_analysis(emotions):\n",
    "    objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    \n",
    "    plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('percentage')\n",
    "    plt.title('emotion')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankurgupta/anaconda3/lib/python3.6/site-packages/keras_preprocessing/image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage import io\n",
    "img = image.load_img('output-onlinepngtools.png', grayscale=True, target_size=(48, 48))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis = 0)\n",
    "\n",
    "x /= 255\n",
    "\n",
    "custom = model.predict(x)\n",
    "\n",
    "emotion_analysis(custom[0])\n",
    "\n",
    "# x = np.array(x, 'float32')\n",
    "# x = x.reshape([48, 48]);\n",
    "\n",
    "# plt.gray()\n",
    "# plt.imshow(x)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
